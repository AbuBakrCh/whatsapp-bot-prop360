import streamlit as st
import pandas as pd
import numpy as np
import cohere
from time import time

# --- Cohere Setup ---
COHERE_API_KEY = "43ne7RbinxXERVHw6P04l2ErDWbAZrEw9ZU6VlSq"
cohere_client = cohere.Client(COHERE_API_KEY)

# --- Load Dataset ---
@st.cache_data
def load_data():
    df = pd.read_csv("faq_tr.csv")
    df = df.dropna(subset=["OLASI SORULAR - PINAR AGENT 35 YAÅINDA ATÄ°NADA YAÅIYOR", "CEVAPLAR "])
    df = df.reset_index(drop=True)
    return df

# --- Build Embedding Index (Cohere only) ---
@st.cache_resource(show_spinner=False)
def build_index(df):
    st.write("ğŸ“¦ Encoding dataset with **Cohere Multilingual v3.0**...")
    start = time()

    texts = df["OLASI SORULAR - PINAR AGENT 35 YAÅINDA ATÄ°NADA YAÅIYOR"].astype(str).tolist()
    all_embeddings = []

    batch_size = 50
    for i in range(0, len(texts), batch_size):
        batch = texts[i:i + batch_size]
        response = cohere_client.embed(
            texts=batch,
            model="embed-multilingual-v3.0",
            input_type="search_document"
        )
        all_embeddings.extend(response.embeddings)

    embeddings = np.array(all_embeddings, dtype="float32")
    elapsed = time() - start
    st.success(f"âœ… Encoded {len(texts)} entries in {elapsed:.2f} seconds")
    return embeddings, texts

# --- Search with Cohere Embeddings ---
def semantic_search(user_query, df, embeddings, texts, top_k=3, threshold=0.5):
    query_vec = cohere_client.embed(
        texts=[user_query],
        model="embed-multilingual-v3.0",
        input_type="search_query"
    ).embeddings[0]

    query_vec = np.array(query_vec, dtype="float32")

    # Compute cosine similarity manually
    sim_scores = np.dot(embeddings, query_vec) / (
        np.linalg.norm(embeddings, axis=1) * np.linalg.norm(query_vec) + 1e-10
    )

    top_idx = np.argsort(sim_scores)[::-1][:top_k]
    results = []
    for i in top_idx:
        q = df.iloc[i]["OLASI SORULAR - PINAR AGENT 35 YAÅINDA ATÄ°NADA YAÅIYOR"]
        a = df.iloc[i]["CEVAPLAR "]
        s = float(sim_scores[i])
        results.append((q, a, s))

    best_score = results[0][2] if results else 0.0
    if best_score < threshold:
        return None, results
    return results[0][1], results

#results[0][0] â†’ question text
#results[0][1] â†’ answer text âœ…
#results[0][2] â†’ similarity score


# --- RAG Generation (Cohere Command Model) ---
def generate_rag_response(user_query, results, chat_history):
    # âœ… Step 1: Check if there are good results from the dataset
    if results and results[0][2] > 0.5:
        context = "\n\n".join([f"Soru: {q}\nCevap: {a}" for q, a, _ in results])
    else:
        # No strong match found â†’ return polite hardcoded response
        polite_reply = (
            "Bu konuda elimde net bir bilgi bulunmuyor. "
            "Size en doÄŸru yanÄ±tÄ± verebilmem iÃ§in lÃ¼tfen soruyu biraz daha detaylandÄ±rÄ±r mÄ±sÄ±nÄ±z?"
        )
        return polite_reply, "âš ï¸ Veri kÃ¼mesinde ilgili iÃ§erik bulunamadÄ±."

    # âœ… Step 2: Use last few turns from chat history (for conversational continuity)
    if chat_history:
        history_str = "\n".join([f"KullanÄ±cÄ±: {u}\nAsistan: {a}" for u, a, _ in chat_history[-3:]])
    else:
        history_str = ""

    # âœ… Step 3: Define the system prompt for the assistant's behavior
    system_prompt = (
        "Sen TÃ¼rkÃ§e konuÅŸan profesyonel bir emlak danÄ±ÅŸmanÄ±sÄ±n. "
        "Verilen baÄŸlamÄ± ve konuÅŸma geÃ§miÅŸini kullanarak "
        "kÄ±sa, net ve doÄŸal bir yanÄ±t ver. "
        "EÄŸer emin deÄŸilsen 'Bundan emin deÄŸilim.' de. "
        "Samimi, gÃ¼ven veren ve ikna edici bir Ã¼slup kullan."
        "Kendi bilgi bankanÄ±zdan cevap vermeyin. Herhangi bir bilgi iÃ§in yalnÄ±zca baÄŸlama gÃ¼venin. BU GERÃ‡EKTEN Ã–NEMLÄ°."
    )

    # âœ… Step 4: Combine everything into the model prompt
    user_prompt = (
        f"GeÃ§miÅŸ:\n{history_str}\n\n"
        f"KullanÄ±cÄ±nÄ±n yeni sorusu: {user_query}\n\n"
        f"BaÄŸlam:\n{context}"
    )

    # âœ… Step 5: Call Cohere only when relevant data is available
    try:
        response = cohere_client.chat(
            model="command-a-03-2025",
            message=user_prompt,
            preamble=system_prompt,
            temperature=0.01,
        )
        return response.text.strip(), context
    except Exception as e:
        return f"âš ï¸ Hata: {str(e)}", context

# --- Streamlit UI ---
st.set_page_config(page_title="TÃ¼rkÃ§e Chatbot ğŸ’¬", page_icon="ğŸ’¬", layout="wide")
st.title("ğŸ’¬ TÃ¼rkÃ§e Chatbot â€“ Cohere RAG SÃ¼rÃ¼mÃ¼")

df = load_data()
embeddings, texts = build_index(df)

# Initialize chat history
if "chat_history" not in st.session_state:
    st.session_state.chat_history = []

user_query = st.chat_input("Sorunuzu yazÄ±n...") #Write your question

if user_query:
    with st.spinner("YanÄ±t hazÄ±rlanÄ±yor..."):  # Response is being prepared
        answer, results = semantic_search(user_query, df, embeddings, texts)

        if answer is None:
            # âœ… No dataset match â†’ return polite static response (no LLM call)
            rag_response = (
                "Bu konuda elimde net bir bilgi bulunmuyor. "
                "Ben yalnÄ±zca **Yunanistan Golden Visa** programÄ± ile ilgili sorulara yardÄ±mcÄ± olabiliyorum. ğŸ‡¬ğŸ‡· "
                "LÃ¼tfen sorunuz bu konuyla ilgiliyse tekrar yazÄ±n. ğŸ˜Š"
            )
            context_used = "âš ï¸ Veri kÃ¼mesinde ilgili iÃ§erik bulunamadÄ±."
        else:
            # âœ… Dataset match â†’ use RAG generation (calls LLM)
            rag_response, context_used = generate_rag_response(
                user_query, results, st.session_state.chat_history
            )

    st.session_state.chat_history.append((user_query, rag_response, context_used))

# --- Display Chat ---
for user_msg, bot_msg, ctx in st.session_state.chat_history:
    with st.chat_message("user"):
        st.markdown(user_msg)
    with st.chat_message("assistant"):
        st.markdown(bot_msg)
        if ctx:
            with st.expander("ğŸ“š GÃ¶rÃ¼len BaÄŸlam (Datasetten AlÄ±nan Bilgi)"):
                st.markdown(f"<div style='font-size:13px; white-space:pre-wrap;'>{ctx}</div>", unsafe_allow_html=True)
